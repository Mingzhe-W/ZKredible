{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilQ4ttNFnXqq"
      },
      "source": [
        "# ZK lending\n",
        "This notebook illustrate the idea of ZK lending.\n",
        "\n",
        "some introduction to be added\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTRHxNAtnXqs"
      },
      "source": [
        "## Setup\n",
        "\n",
        "install and import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFEaWzrIndPa",
        "outputId": "90b266f0-ef78-4a9e-aba6-ef01656bb91c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal\n",
            "Successfully installed tenseal-0.3.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "W9JQgNNxnXqt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tenseal as ts\n",
        "import pandas as pd\n",
        "import random\n",
        "from time import time\n",
        "\n",
        "# those are optional and are not necessary for training\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fo7k607nXqu"
      },
      "source": [
        "We use the dataset for credit score classification from [kaggle](https://www.kaggle.com/datasets/parisrohan/credit-score-classification/data?select=train.csv).\n",
        "\n",
        "We already clean the data and for the purpose of clarity, we won't show the cleaning process here. You can check the data cleaning part in our code base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4M-Nux7nXqv",
        "outputId": "38d1fa2b-8885-4d2b-c2fb-6a68829a26ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Data summary #############\n",
            "x_train has shape: torch.Size([100, 10])\n",
            "y_train has shape: torch.Size([100, 1])\n",
            "x_test has shape: torch.Size([50, 10])\n",
            "y_test has shape: torch.Size([50, 1])\n",
            "#######################################\n"
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(73)\n",
        "random.seed(73)\n",
        "\n",
        "\n",
        "def split_train_test(x, y, test_ratio=0.3):\n",
        "    idxs = [i for i in range(len(x))]\n",
        "    random.shuffle(idxs)\n",
        "    # delimiter between test and train data\n",
        "    delim = int(len(x) * test_ratio)\n",
        "    test_idxs, train_idxs = idxs[:delim], idxs[delim:]\n",
        "    return x[train_idxs], y[train_idxs], x[test_idxs], y[test_idxs]\n",
        "\n",
        "\n",
        "# def heart_disease_data():\n",
        "#     data = pd.read_csv(\"./data/framingham.csv\")\n",
        "#     # drop rows with missing values\n",
        "#     data = data.dropna()\n",
        "#     # drop some features\n",
        "#     data = data.drop(columns=[\"education\", \"currentSmoker\", \"BPMeds\", \"diabetes\", \"diaBP\", \"BMI\"])\n",
        "#     # balance data\n",
        "#     grouped = data.groupby('TenYearCHD')\n",
        "#     data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=73).reset_index(drop=True))\n",
        "#     # extract labels\n",
        "#     y = torch.tensor(data[\"TenYearCHD\"].values).float().unsqueeze(1)\n",
        "#     data = data.drop(\"TenYearCHD\", 'columns')\n",
        "#     # standardize data\n",
        "#     data = (data - data.mean()) / data.std()\n",
        "#     x = torch.tensor(data.values).float()\n",
        "#     return split_train_test(x, y)\n",
        "\n",
        "\n",
        "def random_data(m=100, n=10):\n",
        "    # data separable by the line `y = x`\n",
        "    x_train = torch.randn(m, n)\n",
        "    x_test = torch.randn(m // 2, n)\n",
        "    y_train = (x_train[:, 0] >= x_train[:, 1]).float().unsqueeze(0).t()\n",
        "    y_test = (x_test[:, 0] >= x_test[:, 1]).float().unsqueeze(0).t()\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "# You can use whatever data you want without modification to the tutorial\n",
        "# x_train, y_train, x_test, y_test = random_data()\n",
        "x_train, y_train, x_test, y_test = random_data()\n",
        "\n",
        "print(\"############# Data summary #############\")\n",
        "print(f\"x_train has shape: {x_train.shape}\")\n",
        "print(f\"y_train has shape: {y_train.shape}\")\n",
        "print(f\"x_test has shape: {x_test.shape}\")\n",
        "print(f\"y_test has shape: {y_test.shape}\")\n",
        "print(\"#######################################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbFfipvznXq3"
      },
      "source": [
        "We saw that evaluating on the encrypted test set doesn't affect the accuracy that much. I've even seen examples where the encrypted evaluation performs better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBL-gRBynXq3"
      },
      "source": [
        "## Training an Encrypted Logistic Regression Model on Encrypted Data\n",
        "\n",
        "In this part we will redefine a PyTorch-like model that can both forward encrypted data, as well as backpropagate to update the weights and thus train the encrypted logistic regression model on encrypted data. Below are more details about the training.\n",
        "\n",
        "#### Loss Function\n",
        "\n",
        "We are using the binary cross entropy loss function with regularization (more about the why of regularization will follow) where $y^{(i)}$ is the i'th expected label, $\\hat{y}^{(i)}$ is the i'th output of the logistic regression model and $\\theta$ is our n-sized weight vector.\n",
        "\n",
        "$$Loss(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m [y^{(i)} log(\\hat{y}^{(i)}) + (1 - y^{(i)}) log (1 - \\hat{y}^{(i)})] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$$\n",
        "\n",
        "#### Parameters Update\n",
        "\n",
        "For updating the parameter, the usual rule is as follows, where $x^{(i)}$ is the i'th input data:\n",
        "\n",
        "$$\\theta_j = \\theta_j - \\alpha \\; [ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} + \\frac{\\lambda}{m} \\theta_j]$$\n",
        "\n",
        "However, due to homomorphic encryption constraint, we preferred to use an $\\alpha = 1$ to reduce a multiplication and set $\\frac{\\lambda}{m} = 0.05$ which gets us to the following update rule:\n",
        "\n",
        "$$\\theta_j = \\theta_j - [ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} + 0.05 \\theta_j]$$\n",
        "\n",
        "#### Sigmoid Approximation\n",
        "\n",
        "Since we can't simply compute sigmoid on encrypted data, we need to approximate it using a low degree polynomial, the lower the degree the better, as we aim to perform as few multiplications as possible, to be able to use smaller parameters and thus optimize computation. This tutorial uses a degree 3 polynomial from https://eprint.iacr.org/2018/462.pdf, which approximates the sigmoid function in the range $[-5,5]$.\n",
        "\n",
        "$$\\sigma(x) = 0.5 + 0.197 x - 0.004 x^3$$\n",
        "\n",
        "#### Homomorphic Encryption Parameters\n",
        "\n",
        "From the input data to the parameter update, a ciphertext will need a multiplicative depth of 6, 1 for the dot product operation, 2 for the sigmoid approximation, and 3 for the backpropagation phase (one is actually hidden in the `self._delta_w += enc_x * out_minus_y` operation in the `backward()` function, which is multiplying a 1-sized vector with an n-sized one, which requires masking the first slot and replicating it n times in the first vector). With a scale of around 20 bits, we need 6 coefficients modulus with the same bit-size as the scale, plus the last coefficient, which needs more bits, we are already out of the 4096 polynomial modulus degree (which requires < 109 total bit count of the coefficients modulus, if we consider 128-bit security), so we will use 8192. This will allow us to batch up to 4096 values in a single ciphertext, but we are far away from this limitation, so we shouldn't even think about it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "asz6OsBBnXq4"
      },
      "outputs": [],
      "source": [
        "class EncryptedLR:\n",
        "\n",
        "    def __init__(self, n_features):\n",
        "        model = torch.nn.Linear(n_features, 1)\n",
        "        self.weight = model.weight.data.tolist()[0]\n",
        "        self.bias = model.bias.data.tolist()\n",
        "        # we accumulate gradients and counts the number of iterations\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    def forward(self, enc_x):\n",
        "        enc_out = enc_x.dot(self.weight) + self.bias\n",
        "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
        "        return enc_out\n",
        "\n",
        "    def backward(self, enc_x, enc_out, enc_y):\n",
        "        out_minus_y = (enc_out - enc_y)\n",
        "        self._delta_w += enc_x * out_minus_y\n",
        "        self._delta_b += out_minus_y\n",
        "        self._count += 1\n",
        "\n",
        "    def update_parameters(self):\n",
        "        if self._count == 0:\n",
        "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
        "        # update weights\n",
        "        # We use a small regularization term to keep the output\n",
        "        # of the linear layer in the range of the sigmoid approximation\n",
        "        self.weight -= self._delta_w * (1 / self._count) + self.weight * 0.05\n",
        "        self.bias -= self._delta_b * (1 / self._count)\n",
        "        # reset gradient accumulators and iterations count\n",
        "        self._delta_w = 0\n",
        "        self._delta_b = 0\n",
        "        self._count = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(enc_x):\n",
        "        # We use the polynomial approximation of degree 3\n",
        "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
        "        # from https://eprint.iacr.org/2018/462.pdf\n",
        "        # which fits the function pretty well in the range [-5,5]\n",
        "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
        "\n",
        "    def plain_accuracy(self, x_test, y_test):\n",
        "        # evaluate accuracy of the model on\n",
        "        # the plain (x_test, y_test) dataset\n",
        "        w = torch.tensor(self.weight)\n",
        "        b = torch.tensor(self.bias)\n",
        "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
        "        correct = torch.abs(y_test - out) < 0.5\n",
        "        return correct.float().mean()\n",
        "\n",
        "    def encrypt(self, context):\n",
        "        self.weight = ts.ckks_vector(context, self.weight)\n",
        "        self.bias = ts.ckks_vector(context, self.bias)\n",
        "\n",
        "    def decrypt(self):\n",
        "        self.weight = self.weight.decrypt()\n",
        "        self.bias = self.bias.decrypt()\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "4nSMEp0bnXq4"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "poly_mod_degree = 8192\n",
        "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
        "# create TenSEALContext\n",
        "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
        "ctx_training.global_scale = 2 ** 21\n",
        "ctx_training.generate_galois_keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBR9QhLLnXq4",
        "outputId": "670a4a8d-913c-4758-d6c9-d4931241c736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encryption of the training_set took 2 seconds\n"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "enc_x_train = [ts.ckks_vector(ctx_training, x.tolist()) for x in x_train]\n",
        "enc_y_train = [ts.ckks_vector(ctx_training, y.tolist()) for y in y_train]\n",
        "t_end = time()\n",
        "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Zk9-c1nXq5"
      },
      "source": [
        "Below we study the distribution of `x.dot(weight) + bias` in both plain and encrypted domains. Making sure that it falls into the range $[-5,5]$, which is where our sigmoid approximation is good at, and we don't want to feed it data that is out of this range so that we don't get erroneous output, which can make our training behave unpredictably. But the weights will change during the training process, and we should try to keep them as small as possible while still learning. A technique often used with logistic regression, and we do exactly this (but serving another purpose which is *generalization*), is known as *regularization*, and you might already have spotted the additional term `self.weight * 0.05` in the `update_parameters()` function, which is the result of doing regularization.\n",
        "\n",
        "To recap, since our sigmoid approximation is only good in the range $[-5,5]$, we want to have all its inputs in that range. In order to do this, we need to keep our logistic regression parameters as small as possible, so we apply regularization.\n",
        "\n",
        "**Note:** Keeping the parameters small certainly reduces the magnitude of the output, but we can also get out of range if the data wasn't standardized. You may have spotted that we standardized the data with a mean of 0 and std of 1, this was both for better performance, as well as to keep the inputs to the sigmoid in the desired range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "x1xxfYW2nXq5",
        "outputId": "d1145878-f5af-4e1b-9e41-6742e4a595ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution on encrypted data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "normal_dist = lambda x, mean, var: np.exp(- np.square(x - mean) / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
        "\n",
        "def plot_normal_dist(mean, var, rmin=-10, rmax=10):\n",
        "    x = np.arange(rmin, rmax, 0.01)\n",
        "    y = normal_dist(x, mean, var)\n",
        "    fig = plt.plot(x, y)\n",
        "\n",
        "# plain distribution\n",
        "# lr = LR(n_features)\n",
        "# data = lr.lr(x_test)\n",
        "# mean, var = map(float, [data.mean(), data.std() ** 2])\n",
        "# plot_normal_dist(mean, var)\n",
        "# print(\"Distribution on plain data:\")\n",
        "# plt.show()\n",
        "\n",
        "#encrypted distribution\n",
        "def encrypted_out_distribution(encrypted_logistic, enc_x_test):\n",
        "    w = encrypted_logistic.weight\n",
        "    b = encrypted_logistic.bias\n",
        "    data = []\n",
        "    for enc_x in enc_x_test:\n",
        "        enc_out = enc_x.dot(w) + b\n",
        "        data.append(enc_out.decrypt())\n",
        "    data = torch.tensor(data)\n",
        "    mean, var = map(float, [data.mean(), data.std() ** 2])\n",
        "    plot_normal_dist(mean, var)\n",
        "    print(\"Distribution on encrypted data:\")\n",
        "    plt.show()\n",
        "\n",
        "n_features = x_train.shape[1]\n",
        "\n",
        "encrypted_logistic = EncryptedLR(n_features)\n",
        "encrypted_logistic.encrypt(ctx_training)\n",
        "encrypted_out_distribution(encrypted_logistic, enc_x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHfSgPUjnXq6"
      },
      "source": [
        "Most of the data falls into $[-5,5]$, the sigmoid approximation should be good enough!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsuC486dnXq6"
      },
      "source": [
        "We finally reached the last part, which is about training an encrypted logistic regression model on encrypted data! You can see that we decrypt the weights and re-encrypt them again after every epoch, this is necessary since after updating the weights at the end of the epoch, we can no longer use them to perform enough multiplications, so we need to get them back to the initial ciphertext level. In a real scenario, this would translate to sending the weights back to the secret-key holder for decryption and re-encryption. In that case, it will result in just a few Kilobytes of communication per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH6LdQnsnXq7",
        "outputId": "140f0d5a-cad1-471d-e83d-2b7164629f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy at epoch #1 is 0.9399999976158142\n",
            "[1.015377036120562, -1.1959251731381393, -0.15128247212029075, -0.10031845901031751, -0.04793443171365424, -0.01023744801326075, 0.14224718184126958, -0.4667343890129133, 0.1015768257033605, 0.059186276666773854]\n",
            "[0.24503304160099715]\n"
          ]
        }
      ],
      "source": [
        "encrypted_logistic = EncryptedLR(n_features)\n",
        "# accuracy = eelr.plain_accuracy(x_test, y_test)\n",
        "# print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
        "\n",
        "times = []\n",
        "\n",
        "## temp\n",
        "EPOCHS=1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    encrypted_logistic.encrypt(ctx_training)\n",
        "\n",
        "    # if you want to keep an eye on the distribution to make sure\n",
        "    # the function approximation is still working fine\n",
        "    # WARNING: this operation is time consuming\n",
        "    # encrypted_out_distribution(eelr, enc_x_train)\n",
        "\n",
        "    t_start = time()\n",
        "    for enc_x, enc_y in zip(enc_x_train, enc_y_train):\n",
        "        enc_out = encrypted_logistic.forward(enc_x)\n",
        "        encrypted_logistic.backward(enc_x, enc_out, enc_y)\n",
        "    encrypted_logistic.update_parameters()\n",
        "    t_end = time()\n",
        "    times.append(t_end - t_start)\n",
        "\n",
        "    encrypted_logistic.decrypt()\n",
        "    accuracy = encrypted_logistic.plain_accuracy(x_test, y_test)\n",
        "    print(f\"Accuracy at epoch #{epoch + 1} is {accuracy}\")\n",
        "\n",
        "\n",
        "print(encrypted_logistic.weight)\n",
        "print(encrypted_logistic.bias)\n",
        "\n",
        "\n",
        "# print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
        "# print(f\"Final accuracy is {accuracy}\")\n",
        "\n",
        "# diff_accuracy = plain_accuracy - accuracy\n",
        "# print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
        "# if diff_accuracy < 0:\n",
        "#     print(\"Oh! We got a better accuracy when training on encrypted data! The noise was on our side...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparision with plain Logistic Regression Model\n",
        "Now let's train a plain logistic regression model with pytorch and compare the result with model trained with homomorphic encryption"
      ],
      "metadata": {
        "id": "MaYP4kHzyocK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_features):\n",
        "        super(LogisticModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(n_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.sigmoid(self.linear(x))\n",
        "        return out\n",
        "\n",
        "# defining model, optimizer and loss function\n",
        "n_features = x_train.shape[1]\n",
        "plain_logistic = LogisticModel(n_features)\n",
        "optim = torch.optim.SGD(plain_logistic.parameters(), lr=1)\n",
        "lossf = torch.nn.BCELoss()\n",
        "\n",
        "EPOCHS =5\n",
        "\n",
        "# training process\n",
        "for epoch in range(EPOCHS):\n",
        "    optim.zero_grad()\n",
        "    out = plain_logistic(x_train)\n",
        "    loss = lossf(out, y_train)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    print(f\"Loss at epoch {epoch + 1}: {loss.data}\")\n",
        "\n",
        "\n",
        "# print(plain_logistic.linear.weight)\n",
        "# print(plain_logistic.linear.bias)\n",
        "\n",
        "# calculate the accuracy\n",
        "def accuracy(model, x, y):\n",
        "    out = model(x)\n",
        "    correct = torch.abs(y - out) < 0.5\n",
        "    return correct.float().mean()\n",
        "\n",
        "plain_accuracy = accuracy(plain_logistic, x_test, y_test)\n",
        "print(f\"Accuracy of plain model on test_set: {plain_accuracy}\")\n",
        "\n",
        "print(plain_logistic.linear.weight)\n",
        "\n",
        "print(plain_logistic.linear.bias)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k8pczKf4Ho6",
        "outputId": "14f99fca-655a-4d7a-da40-38ca9e9e2491"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch 1: 0.725821852684021\n",
            "Loss at epoch 2: 0.49990516901016235\n",
            "Loss at epoch 3: 0.3958192467689514\n",
            "Loss at epoch 4: 0.33897164463996887\n",
            "Loss at epoch 5: 0.3026331067085266\n",
            "Accuracy of plain model on test_set: 0.9399999976158142\n",
            "Parameter containing:\n",
            "tensor([[ 1.0065, -0.9780, -0.0405,  0.0571,  0.1674, -0.0064, -0.0295, -0.4292,\n",
            "          0.0449,  0.1444]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0667], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wf6FTtAnXq8"
      },
      "source": [
        "# Convert encrypted model to ONNX format and thus can be feed into EZKL\n",
        "\n",
        "Now we do a conversion, unfortunately there is no elegant way to do so, as tenseal does not natively support exporting to ONNX format. So we have to convert it first to a pytorch logistic model, by manually assigned it's weight and bias to what we just trained in encrypted model, and then export it using pytorch's api\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_EZKL = LogisticModel(n_features)\n",
        "print(model_to_EZKL.linear.weight)\n",
        "print(model_to_EZKL.linear.bias)\n",
        "model_to_EZKL.linear.weight.data = torch.tensor([encrypted_logistic.weight])\n",
        "model_to_EZKL.linear.bias.data = torch.tensor(encrypted_logistic.bias)\n",
        "\n",
        "print(model_to_EZKL.linear.weight)\n",
        "print(model_to_EZKL.linear.bias)\n",
        "\n",
        "print(plain_logistic.linear.weight)\n",
        "print(plain_logistic.linear.bias)\n",
        "\n",
        "test_accuracy = accuracy(model_to_EZKL, x_test, y_test)\n",
        "print(f\"Accuracy of plain to ezkl on test_set: {test_accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWp-LmABHHj8",
        "outputId": "3b9b5877-7f03-464c-f0e3-b4c284c6343a"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0004, -0.1120,  0.0045, -0.2022,  0.0072, -0.0397,  0.2048, -0.1929,\n",
            "          0.3028, -0.1658]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1143], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 1.0154, -1.1959, -0.1513, -0.1003, -0.0479, -0.0102,  0.1422, -0.4667,\n",
            "          0.1016,  0.0592]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.2450], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[ 1.0065, -0.9780, -0.0405,  0.0571,  0.1674, -0.0064, -0.0295, -0.4292,\n",
            "          0.0449,  0.1444]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0667], requires_grad=True)\n",
            "Accuracy of plain to ezkl on test_set: 0.9399999976158142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXAR9J2WHIfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}